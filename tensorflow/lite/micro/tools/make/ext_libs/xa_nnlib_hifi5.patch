diff --git a/algo/common/include/xa_nnlib_common_macros_hifi5.h b/algo/common/include/xa_nnlib_common_macros_hifi5.h
index 9ad396d..f75781c 100644
--- a/algo/common/include/xa_nnlib_common_macros_hifi5.h
+++ b/algo/common/include/xa_nnlib_common_macros_hifi5.h
@@ -1343,8 +1343,8 @@ ae_int16 _ae_int16_bias = ZERO16; \
           AE_MULA8Q8X8(_ae_int32x2_acc_1, _ae_int32x2_acc_0,zero_temp,zero_temp,zero_temp,_ae_int8x8_mat1_1_0,_ae_int8x8_vec1_1);
 
 #define KERNEL_8x8_NOT_UNROLLED_MAT2_VEC2 \
-          AE_MULA8Q8X8(_ae_int32x2_acc_0, _ae_int32x2_acc_0,zero_temp,zero_temp,zero_temp,_ae_int8x8_mat2_0,_ae_int8x8_vec2);\
-          AE_MULA8Q8X8(_ae_int32x2_acc_0, _ae_int32x2_acc_0,zero_temp,zero_temp,zero_temp,_ae_int8x8_mat2_1_0,_ae_int8x8_vec2_1);
+          AE_MULA8Q8X8(_ae_int32x2_acc_1, _ae_int32x2_acc_0,zero_temp,zero_temp,zero_temp,_ae_int8x8_mat2_0,_ae_int8x8_vec2);\
+          AE_MULA8Q8X8(_ae_int32x2_acc_1, _ae_int32x2_acc_0,zero_temp,zero_temp,zero_temp,_ae_int8x8_mat2_1_0,_ae_int8x8_vec2_1);
 
 #define STORE_ACC_8bx8b_AT_OUT_8_SINGLE \
         ae_int8x8 temp;\
diff --git a/algo/common/include/xa_nnlib_quant_macros_hifi5.h b/algo/common/include/xa_nnlib_quant_macros_hifi5.h
index 54e3dde..0573447 100644
--- a/algo/common/include/xa_nnlib_quant_macros_hifi5.h
+++ b/algo/common/include/xa_nnlib_quant_macros_hifi5.h
@@ -342,10 +342,11 @@
 }
 
 #define MPY_BY_QUANT_MULT_X2X2_OUT16_ZB(out, inp1, inp2, multiplier, l_shift, r_shift, out_off) \
-  AE_MUL2P32X4S(inp1, inp2, inp1, inp2, AE_SLAA32S(AE_MOVI(1), l_shift), AE_SLAA32S(AE_MOVI(1), l_shift)); \
-  AE_MULF2P32X4RAS(inp1, inp2, inp1, inp2, AE_MOVDA32(multiplier), AE_MOVDA32(multiplier)); \
-  AE_MULF2P32X4RS(inp1, inp2, inp1, inp2, AE_SRAA32S(AE_MOVDA32(0x80000000), r_shift), AE_SRAA32S(AE_MOVDA32(0x80000000), r_shift));      \
-  out = AE_SAT16X4(inp1, inp2); \
+  ae_int32x2 temp1_ ## inp1, temp2_ ## inp2; \
+  AE_MUL2P32X4S(temp1_ ## inp1, temp2_ ## inp2, inp1, inp2, AE_SLAA32S(AE_MOVI(1), l_shift), AE_SLAA32S(AE_MOVI(1), l_shift)); \
+  AE_MULF2P32X4RAS(inp1, inp2, temp1_ ## inp1, temp2_ ## inp2, AE_MOVDA32(multiplier), AE_MOVDA32(multiplier)); \
+  AE_MULF2P32X4RS(temp1_ ## inp1, temp2_ ## inp2, inp1, inp2, AE_SRAA32S(AE_MOVDA32(0x80000000), r_shift), AE_SRAA32S(AE_MOVDA32(0x80000000), r_shift));      \
+  out = AE_SAT16X4(temp1_ ## inp1, temp2_ ## inp2); \
   out = AE_SUB16S(AE_MOVDA16(out_off), out);
 
 #define MPY_BY_QUANT_MULT_SLS_X2X2_OUT16_ZB(out, inp1, inp2, multiplier, l_shift, r_shift, out_off) \
@@ -360,10 +361,11 @@
 
 #define MPY_BY_QUANT_MULT_PER_CHAN_X2X2_OUT16_ZB(out, inp1, inp2, mult_01, mult_23, ls_01, ls_23, rs_01, rs_23, out_off) \
 {\
-  AE_MUL2P32X4S(inp1, inp2, inp1, inp2, AE_SRAV32RS(AE_MOVI(1), AE_NEG32(ls_01)), AE_SRAV32RS(AE_MOVI(1), AE_NEG32(ls_23))); \
-  AE_MULF2P32X4RAS(inp1, inp2, inp1, inp2, mult_01, mult_23); \
-  AE_MULF2P32X4RS(inp1, inp2, inp1, inp2, AE_SRAV32RS(AE_MOVDA32(0x80000000), rs_01), AE_SRAV32RS(AE_MOVDA32(0x80000000), rs_23)); \
-  out = AE_SAT16X4(inp1, inp2); \
+  ae_int32x2 temp1_ ## inp1, temp2_ ## inp2; \
+  AE_MUL2P32X4S(temp1_ ## inp1, temp2_ ## inp2, inp1, inp2, AE_SRAV32RS(AE_MOVI(1), AE_NEG32(ls_01)), AE_SRAV32RS(AE_MOVI(1), AE_NEG32(ls_23))); \
+  AE_MULF2P32X4RAS(inp1, inp2, temp1_ ## inp1, temp2_ ## inp2, mult_01, mult_23); \
+  AE_MULF2P32X4RS(temp1_ ## inp1, temp2_ ## inp2, inp1, inp2, AE_SRAV32RS(AE_MOVDA32(0x80000000), rs_01), AE_SRAV32RS(AE_MOVDA32(0x80000000), rs_23)); \
+  out = AE_SAT16X4(temp1_ ## inp1, temp2_ ## inp2); \
   out = AE_SUB16S(AE_MOVDA16(out_off), out); \
 }
 
diff --git a/algo/kernels/cnn/hifi5/xa_nn_matXvec_sym4sxasym8s_asym8s_circ.c b/algo/kernels/cnn/hifi5/xa_nn_matXvec_sym4sxasym8s_asym8s_circ.c
index 7d80a73..e6f0f95 100644
--- a/algo/kernels/cnn/hifi5/xa_nn_matXvec_sym4sxasym8s_asym8s_circ.c
+++ b/algo/kernels/cnn/hifi5/xa_nn_matXvec_sym4sxasym8s_asym8s_circ.c
@@ -1180,7 +1180,7 @@ WORD32 xa_nn_matXvec_sym4sxasym8s_asym8s_circ(
       ae_int32x2 acc2 = AE_ZERO32();
       ae_int32x2 acc3 = AE_ZERO32();  
 
-      ae_int32x2 dummy_acc;
+      ae_int32x2 dummy_acc, dummy_acc_;
       ae_int8x8 mat0_0, mat0_1, mat0_2, mat0_3;
       ae_int8x8 mat1_0, mat1_1, mat1_2, mat1_3;
       ae_int8x8 vec0_0, vec0_1;
@@ -1242,15 +1242,15 @@ WORD32 xa_nn_matXvec_sym4sxasym8s_asym8s_circ(
         AE_SUBW8(mat_plus_zb1_4, mat_plus_zb1_5, mat1_2, mat_zb);
         AE_SUBW8(mat_plus_zb1_6, mat_plus_zb1_7, mat1_3, mat_zb);
 
-        AE_MULA4O4X16(acc0, dummy_acc, acc1, dummy_acc, vec_4b_0_0, vec_4b_0_0, mat_plus_zb0_0, mat_plus_zb0_1);
-        AE_MULA4O4X16(dummy_acc, acc0, dummy_acc, acc1, vec_4b_0_0, vec_4b_0_0, mat_plus_zb0_2, mat_plus_zb0_3);
-        AE_MULA4O4X16(acc0, dummy_acc, acc1, dummy_acc, vec_4b_0_1, vec_4b_0_1, mat_plus_zb0_4, mat_plus_zb0_5);
-        AE_MULA4O4X16(dummy_acc, acc0, dummy_acc, acc1, vec_4b_0_1, vec_4b_0_1, mat_plus_zb0_6, mat_plus_zb0_7);
+        AE_MULA4O4X16(acc0, dummy_acc_, acc1, dummy_acc, vec_4b_0_0, vec_4b_0_0, mat_plus_zb0_0, mat_plus_zb0_1);
+        AE_MULA4O4X16(dummy_acc_, acc0, dummy_acc, acc1, vec_4b_0_0, vec_4b_0_0, mat_plus_zb0_2, mat_plus_zb0_3);
+        AE_MULA4O4X16(acc0, dummy_acc_, acc1, dummy_acc, vec_4b_0_1, vec_4b_0_1, mat_plus_zb0_4, mat_plus_zb0_5);
+        AE_MULA4O4X16(dummy_acc_, acc0, dummy_acc, acc1, vec_4b_0_1, vec_4b_0_1, mat_plus_zb0_6, mat_plus_zb0_7);
 
-        AE_MULA4O4X16(acc2, dummy_acc, acc3, dummy_acc, vec_4b_0_0, vec_4b_0_0, mat_plus_zb1_0, mat_plus_zb1_1);
-        AE_MULA4O4X16(dummy_acc, acc2, dummy_acc, acc3, vec_4b_0_0, vec_4b_0_0, mat_plus_zb1_2, mat_plus_zb1_3);
-        AE_MULA4O4X16(acc2, dummy_acc, acc3, dummy_acc, vec_4b_0_1, vec_4b_0_1, mat_plus_zb1_4, mat_plus_zb1_5);
-        AE_MULA4O4X16(dummy_acc, acc2, dummy_acc, acc3, vec_4b_0_1, vec_4b_0_1, mat_plus_zb1_6, mat_plus_zb1_7);        
+        AE_MULA4O4X16(acc2, dummy_acc_, acc3, dummy_acc, vec_4b_0_0, vec_4b_0_0, mat_plus_zb1_0, mat_plus_zb1_1);
+        AE_MULA4O4X16(dummy_acc_, acc2, dummy_acc, acc3, vec_4b_0_0, vec_4b_0_0, mat_plus_zb1_2, mat_plus_zb1_3);
+        AE_MULA4O4X16(acc2, dummy_acc_, acc3, dummy_acc, vec_4b_0_1, vec_4b_0_1, mat_plus_zb1_4, mat_plus_zb1_5);
+        AE_MULA4O4X16(dummy_acc_, acc2, dummy_acc, acc3, vec_4b_0_1, vec_4b_0_1, mat_plus_zb1_6, mat_plus_zb1_7);
       }        
       if(cols_count != cols1)
       {
@@ -1275,15 +1275,15 @@ WORD32 xa_nn_matXvec_sym4sxasym8s_asym8s_circ(
         AE_SUBW8(mat_plus_zb1_4, mat_plus_zb1_5, mat1_2, mat_zb);
         AE_SUBW8(mat_plus_zb1_6, mat_plus_zb1_7, mat1_3, mat_zb);
 
-        AE_MULA4O4X16(acc0, dummy_acc, acc1, dummy_acc, vec_4b_0_0, vec_4b_0_0, mat_plus_zb0_0, mat_plus_zb0_1);
-        AE_MULA4O4X16(dummy_acc, acc0, dummy_acc, acc1, vec_4b_0_0, vec_4b_0_0, mat_plus_zb0_2, mat_plus_zb0_3);
-        AE_MULA4O4X16(acc0, dummy_acc, acc1, dummy_acc, vec_4b_0_1, vec_4b_0_1, mat_plus_zb0_4, mat_plus_zb0_5);
-        AE_MULA4O4X16(dummy_acc, acc0, dummy_acc, acc1, vec_4b_0_1, vec_4b_0_1, mat_plus_zb0_6, mat_plus_zb0_7);
+        AE_MULA4O4X16(acc0, dummy_acc_, acc1, dummy_acc, vec_4b_0_0, vec_4b_0_0, mat_plus_zb0_0, mat_plus_zb0_1);
+        AE_MULA4O4X16(dummy_acc_, acc0, dummy_acc, acc1, vec_4b_0_0, vec_4b_0_0, mat_plus_zb0_2, mat_plus_zb0_3);
+        AE_MULA4O4X16(acc0, dummy_acc_, acc1, dummy_acc, vec_4b_0_1, vec_4b_0_1, mat_plus_zb0_4, mat_plus_zb0_5);
+        AE_MULA4O4X16(dummy_acc_, acc0, dummy_acc, acc1, vec_4b_0_1, vec_4b_0_1, mat_plus_zb0_6, mat_plus_zb0_7);
 
-        AE_MULA4O4X16(acc2, dummy_acc, acc3, dummy_acc, vec_4b_0_0, vec_4b_0_0, mat_plus_zb1_0, mat_plus_zb1_1);
-        AE_MULA4O4X16(dummy_acc, acc2, dummy_acc, acc3, vec_4b_0_0, vec_4b_0_0, mat_plus_zb1_2, mat_plus_zb1_3);
-        AE_MULA4O4X16(acc2, dummy_acc, acc3, dummy_acc, vec_4b_0_1, vec_4b_0_1, mat_plus_zb1_4, mat_plus_zb1_5);
-        AE_MULA4O4X16(dummy_acc, acc2, dummy_acc, acc3, vec_4b_0_1, vec_4b_0_1, mat_plus_zb1_6, mat_plus_zb1_7);  
+        AE_MULA4O4X16(acc2, dummy_acc_, acc3, dummy_acc, vec_4b_0_0, vec_4b_0_0, mat_plus_zb1_0, mat_plus_zb1_1);
+        AE_MULA4O4X16(dummy_acc_, acc2, dummy_acc, acc3, vec_4b_0_0, vec_4b_0_0, mat_plus_zb1_2, mat_plus_zb1_3);
+        AE_MULA4O4X16(acc2, dummy_acc_, acc3, dummy_acc, vec_4b_0_1, vec_4b_0_1, mat_plus_zb1_4, mat_plus_zb1_5);
+        AE_MULA4O4X16(dummy_acc_, acc2, dummy_acc, acc3, vec_4b_0_1, vec_4b_0_1, mat_plus_zb1_6, mat_plus_zb1_7);
       }
       
       acc0 = AE_ADD32S_HL_LH(acc0, acc1);
@@ -1318,6 +1318,7 @@ WORD32 xa_nn_matXvec_sym4sxasym8s_asym8s_circ(
       ae_int32x2 acc0 = AE_ZERO32();
       ae_int32x2 acc1 = AE_ZERO32();     
       ae_int32x2 dummy_acc = AE_ZERO32();
+      ae_int32x2 dummy_acc_ = AE_ZERO32();
       ae_int8x8 mat0, mat1, mat2, mat3;
       ae_int8x8 vec0, vec1;
       ae_int16x4 mat_plus_zb0, mat_plus_zb1;
@@ -1354,10 +1355,10 @@ WORD32 xa_nn_matXvec_sym4sxasym8s_asym8s_circ(
         AE_SUBW8(mat_plus_zb2, mat_plus_zb3, mat1, mat_zb);
         AE_SUBW8(mat_plus_zb4, mat_plus_zb5, mat2, mat_zb);
         AE_SUBW8(mat_plus_zb6, mat_plus_zb7, mat3, mat_zb);
-        AE_MULA4O4X16(acc0, dummy_acc, acc1, dummy_acc, vec_4b_0, vec_4b_0, mat_plus_zb0, mat_plus_zb1);
-        AE_MULA4O4X16(dummy_acc, acc0, dummy_acc, acc1, vec_4b_0, vec_4b_0, mat_plus_zb2, mat_plus_zb3);
-        AE_MULA4O4X16(acc0, dummy_acc, acc1, dummy_acc, vec_4b_1, vec_4b_1, mat_plus_zb4, mat_plus_zb5);
-        AE_MULA4O4X16(dummy_acc, acc0, dummy_acc, acc1, vec_4b_1, vec_4b_1, mat_plus_zb6, mat_plus_zb7);          
+        AE_MULA4O4X16(acc0, dummy_acc_, acc1, dummy_acc, vec_4b_0, vec_4b_0, mat_plus_zb0, mat_plus_zb1);
+        AE_MULA4O4X16(dummy_acc_, acc0, dummy_acc, acc1, vec_4b_0, vec_4b_0, mat_plus_zb2, mat_plus_zb3);
+        AE_MULA4O4X16(acc0, dummy_acc_, acc1, dummy_acc, vec_4b_1, vec_4b_1, mat_plus_zb4, mat_plus_zb5);
+        AE_MULA4O4X16(dummy_acc_, acc0, dummy_acc, acc1, vec_4b_1, vec_4b_1, mat_plus_zb6, mat_plus_zb7);
       }
       if(cols_count != cols1)
       {
@@ -1372,10 +1373,10 @@ WORD32 xa_nn_matXvec_sym4sxasym8s_asym8s_circ(
         AE_SUBW8(mat_plus_zb2, mat_plus_zb3, mat1, mat_zb);
         AE_SUBW8(mat_plus_zb4, mat_plus_zb5, mat2, mat_zb);
         AE_SUBW8(mat_plus_zb6, mat_plus_zb7, mat3, mat_zb);
-        AE_MULA4O4X16(acc0, dummy_acc, acc1, dummy_acc, vec_4b_0, vec_4b_0, mat_plus_zb0, mat_plus_zb1);
-        AE_MULA4O4X16(dummy_acc, acc0, dummy_acc, acc1, vec_4b_0, vec_4b_0, mat_plus_zb2, mat_plus_zb3);
-        AE_MULA4O4X16(acc0, dummy_acc, acc1, dummy_acc, vec_4b_1, vec_4b_1, mat_plus_zb4, mat_plus_zb5);
-        AE_MULA4O4X16(dummy_acc, acc0, dummy_acc, acc1, vec_4b_1, vec_4b_1, mat_plus_zb6, mat_plus_zb7);     
+        AE_MULA4O4X16(acc0, dummy_acc_, acc1, dummy_acc, vec_4b_0, vec_4b_0, mat_plus_zb0, mat_plus_zb1);
+        AE_MULA4O4X16(dummy_acc_, acc0, dummy_acc, acc1, vec_4b_0, vec_4b_0, mat_plus_zb2, mat_plus_zb3);
+        AE_MULA4O4X16(acc0, dummy_acc_, acc1, dummy_acc, vec_4b_1, vec_4b_1, mat_plus_zb4, mat_plus_zb5);
+        AE_MULA4O4X16(dummy_acc_, acc0, dummy_acc, acc1, vec_4b_1, vec_4b_1, mat_plus_zb6, mat_plus_zb7);
       }
       acc = AE_ADD32S_HL_LH(acc0, acc1);
       acc = AE_ADD32S(acc, AE_MOVDA32(p_bias[vec_itr]));
diff --git a/algo/kernels/cnn/hifi5/xa_nn_transpose_conv_sym8sxasym8s.c b/algo/kernels/cnn/hifi5/xa_nn_transpose_conv_sym8sxasym8s.c
index 67ac760..4aaf9f7 100644
--- a/algo/kernels/cnn/hifi5/xa_nn_transpose_conv_sym8sxasym8s.c
+++ b/algo/kernels/cnn/hifi5/xa_nn_transpose_conv_sym8sxasym8s.c
@@ -342,7 +342,7 @@ static inline void tconv2d_sym8sxasym8s(WORD8* output_data,
     {
       acc0 = AE_ADD32(acc0, dbias0);
       ae_int16x4 out16;
-      MPY_BY_QUANT_MULT_PER_CHAN_X2X2_OUT16_ZB(out16, acc0, acc0, p_out_mult01, p_out_mult01, ls_01, ls_01, rs_01, rs_01, output_offset);
+      MPY_BY_QUANT_MULT_PER_CHAN_X2X2_OUT16_ZB(out16, acc1, acc0, p_out_mult01, p_out_mult01, ls_01, ls_01, rs_01, rs_01, output_offset);
       AE_MINMAX16(out16, min_out_16, max_out_16);
       ae_int8x8 d1 = AE_SAT8X8X16(out16, out16);
       AE_S8_0_X(d1, (ae_int8 *) pout0, 1);
@@ -387,7 +387,7 @@ static inline void tconv2d_sym8sxasym8s(WORD8* output_data,
     {
       acc1 = AE_ADD32(acc1, dbias0);
       ae_int16x4 out16;
-      MPY_BY_QUANT_MULT_X2X2_OUT16_ZB(out16, acc1, acc1, output_multiplier[out_channel], left_shift, right_shift, output_offset);
+      MPY_BY_QUANT_MULT_X2X2_OUT16_ZB(out16, acc0, acc1, output_multiplier[out_channel], left_shift, right_shift, output_offset);
       AE_MINMAX16(out16, min_out_16, max_out_16);
       ae_int8x8 d1 = AE_SAT8X8X16(out16, out16);
       AE_S8_0_I(d1, pout1, 0);
diff --git a/algo/kernels/matXvec/hifi5/xa_nn_matmul_asym8sxasym8s.c b/algo/kernels/matXvec/hifi5/xa_nn_matmul_asym8sxasym8s.c
index adc9859..cb182cf 100644
--- a/algo/kernels/matXvec/hifi5/xa_nn_matmul_asym8sxasym8s.c
+++ b/algo/kernels/matXvec/hifi5/xa_nn_matmul_asym8sxasym8s.c
@@ -1688,7 +1688,7 @@ WORD32 xa_nn_matmul_v2_asym8sxasym8s_asym8s(
 
         ae_int16x4 out_0;
 
-        MPY_BY_QUANT_MULT_X2X2_OUT16_ZB(out_0, acc_row0_vec0, acc_row0_vec0, out_multiplier, left_shift, right_shift, out_zero_bias);
+        MPY_BY_QUANT_MULT_X2X2_OUT16_ZB(out_0, acc_row0_vec0, acc_row0_vec1, out_multiplier, left_shift, right_shift, out_zero_bias);
         AE_MINMAX16(out_0, AE_MOVDA16(out_activation_min), AE_MOVDA16(out_activation_max));
 
         ae_int8x8 temp_vec0;
@@ -2173,15 +2173,16 @@ WORD32 xa_nn_matmul_asym8sxasym8s_asym8s(
       for(vec_itr = 0; vec_itr < vec_count; vec_itr++)
       { 
         ae_int32x2 acc_vec0 = bias;
+        ae_int32x2 acc_vec1 = bias;
         
         AE_LAV8X8X2_XP(vec0_0, vec0_1, align_p_vec_0, p_vec_0, cols1);
         
-        MAT_VEC_MAC(acc_vec0 , acc_vec0 , mat1_row3_0 , mat1_row3_0 , mat1_row3_0 , mat1_row3_0 ,vec0_0, -mat1_zero_bias, -vec1_zero_bias);
-        MAT_VEC_MAC(acc_vec0 , acc_vec0 , mat1_row3_1 , mat1_row3_1 , mat1_row3_1 , mat1_row3_1 ,vec0_1, -mat1_zero_bias, -vec1_zero_bias);
+        MAT_VEC_MAC(acc_vec0 , acc_vec1 , mat1_row3_0 , mat1_row3_0 , mat1_row3_0 , mat1_row3_0 ,vec0_0, -mat1_zero_bias, -vec1_zero_bias);
+        MAT_VEC_MAC(acc_vec0 , acc_vec1 , mat1_row3_1 , mat1_row3_1 , mat1_row3_1 , mat1_row3_1 ,vec0_1, -mat1_zero_bias, -vec1_zero_bias);
        
         ae_int16x4 out_0;
         
-        MPY_BY_QUANT_MULT_X2X2_OUT16_ZB(out_0, acc_vec0, acc_vec0, out_multiplier, left_shift, right_shift, out_zero_bias);
+        MPY_BY_QUANT_MULT_X2X2_OUT16_ZB(out_0, acc_vec0, acc_vec1, out_multiplier, left_shift, right_shift, out_zero_bias);
 
         AE_MINMAX16(out_0, AE_MOVDA16(-128), AE_MOVDA16(127));        
         ae_int8x8 temp_vec0 = AE_SAT8X8X16(out_0, out_0);
@@ -2541,7 +2542,7 @@ WORD32 xa_nn_matmul_asym8sxasym8s_asym8s(
 
         ae_int16x4 out_0;
 
-        MPY_BY_QUANT_MULT_X2X2_OUT16_ZB(out_0, acc_row0_vec0, acc_row0_vec0, out_multiplier, left_shift, right_shift, out_zero_bias);
+        MPY_BY_QUANT_MULT_X2X2_OUT16_ZB(out_0, acc_row0_vec0, acc_row0_vec1, out_multiplier, left_shift, right_shift, out_zero_bias);
         AE_MINMAX16(out_0, AE_MOVDA16(-128), AE_MOVDA16(127));
 
         ae_int8x8 temp_vec0;
